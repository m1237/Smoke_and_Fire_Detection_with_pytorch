{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from 'c:\\\\users\\\\pc\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\helper\\\\__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "import helper\n",
    "from importlib import reload\n",
    "reload(helper)\n",
    "#from helper import imshow_original, imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/PC/Desktop/Dataset_FNS/NewDataset_19_11_2019'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "#test_dir = data_dir + '/test'\n",
    "test_dir = 'C:/Users/PC/Desktop/FireNeutralSmoke_C++/mnist_c++/Dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(size=80),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.CenterCrop(80),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=validation_transforms)\n",
    "\n",
    "# Using the image datasets and the transforms, define the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64, num_workers=4)\n",
    "valid_dataloader = DataLoader(validation_dataset, shuffle=True, batch_size=64, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=64, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = sorted(cat_to_name.items(), key=lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'fire'), ('2', 'neutral'), ('3', 'smoke')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = [cat[1] for cat in category_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fire', 'neutral', 'smoke']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(os.listdir(os.path.join(\"NewDataset_19_11_2019\", \"train\")))\n",
    "\n",
    "count = [len(os.listdir(os.path.join(\"NewDataset_19_11_2019\", \"train\", dir))) for dir in os.listdir(os.path.join(\"NewDataset_19_11_2019\", \"train\"))]\n",
    "scalars = [*range(len(os.listdir(os.path.join(\"NewDataset_19_11_2019\", \"train\"))))]\n",
    "scalars = [x.__add__(1) for x in scalars]\n",
    "plt.figure(figsize=(3,6))\n",
    "plt.barh(scalars, count)\n",
    "plt.yticks(scalars, category_names)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Class Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1440, 6995, 20696]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(25088, 4096)),\n",
    "                          ('relu1', nn.ReLU()),\n",
    "                          ('dropout1', nn.Dropout(p=0.5)),\n",
    "                          ('fc2', nn.Linear(4096, 1024)),\n",
    "                          ('relu2', nn.ReLU()),\n",
    "                          ('dropout2', nn.Dropout(p=0.5)),\n",
    "                          ('fc3', nn.Linear(1024, 3)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "vgg16.classifier = classifier\n",
    "vgg16.class_idx_mapping = train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (relu2): ReLU()\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (fc3): Linear(in_features=1024, out_features=3, bias=True)\n",
       "    (output): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion, device):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    model.to(device)\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # images.resize_(images.shape[0], 3, 224, 224)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, validloader, epochs, print_every, criterion, optimizer, device='cuda'):\n",
    "    steps = 0\n",
    "    \n",
    "    # Change to train mode if not already\n",
    "    model.train()\n",
    "    # change to cuda\n",
    "    model.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        for (images, labels) in trainloader:\n",
    "            steps += 1\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            outputs = model.forward(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                \n",
    "                # Make sure network is in eval mode for inference\n",
    "                model.eval()\n",
    "\n",
    "                # Turn off gradients for validation, saves memory and computations\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, accuracy = validation(model, validloader, criterion, device)\n",
    "                    train_loss, train_accuracy = validation(model, trainloader, criterion, device)\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      #\"Validation Loss: {:.3f}.. \".format(validation_loss/len(validloader)),\n",
    "                      \"Train Accuracy: {:.3f}\".format((train_accuracy/len(trainloader))*100),\n",
    "                      \"Validation Accuracy: {:.3f}\".format((accuracy/len(validloader))*100))\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "                running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 0.474..  Train Accuracy: 85.562 Validation Accuracy: 91.895\n",
      "Epoch: 1/2..  Training Loss: 0.360..  Train Accuracy: 86.943 Validation Accuracy: 91.136\n",
      "Epoch: 1/2..  Training Loss: 0.304..  Train Accuracy: 87.823 Validation Accuracy: 91.131\n",
      "Epoch: 1/2..  Training Loss: 0.302..  Train Accuracy: 87.344 Validation Accuracy: 90.594\n",
      "Epoch: 1/2..  Training Loss: 0.278..  Train Accuracy: 88.962 Validation Accuracy: 91.191\n",
      "Epoch: 1/2..  Training Loss: 0.276..  Train Accuracy: 88.481 Validation Accuracy: 91.094\n",
      "Epoch: 1/2..  Training Loss: 0.280..  Train Accuracy: 89.539 Validation Accuracy: 91.577\n",
      "Epoch: 2/2..  Training Loss: 0.111..  Train Accuracy: 89.453 Validation Accuracy: 91.082\n",
      "Epoch: 2/2..  Training Loss: 0.260..  Train Accuracy: 89.258 Validation Accuracy: 91.759\n",
      "Epoch: 2/2..  Training Loss: 0.260..  Train Accuracy: 90.307 Validation Accuracy: 91.320\n",
      "Epoch: 2/2..  Training Loss: 0.258..  Train Accuracy: 88.538 Validation Accuracy: 91.211\n",
      "Epoch: 2/2..  Training Loss: 0.252..  Train Accuracy: 90.082 Validation Accuracy: 91.699\n",
      "Epoch: 2/2..  Training Loss: 0.246..  Train Accuracy: 90.255 Validation Accuracy: 91.602\n",
      "Epoch: 2/2..  Training Loss: 0.241..  Train Accuracy: 90.789 Validation Accuracy: 91.455\n",
      "Epoch: 2/2..  Training Loss: 0.253..  Train Accuracy: 90.698 Validation Accuracy: 91.387\n"
     ]
    }
   ],
   "source": [
    "train(model=vgg16, \n",
    "        trainloader=train_dataloader, \n",
    "        validloader=valid_dataloader,\n",
    "        epochs=2, \n",
    "        print_every=60, \n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_on_test(testloader, model):    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = check_accuracy_on_test(test_dataloader, vgg16)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.ImageFolder('C:/Users/PC/Desktop/FireNeutralSmoke_C++/mnist_c++/Dataset/valid', transform=validation_transforms)\n",
    "#test_dataset = datasets.ImageFolder('/home/a2-murat/Desktop/Dataset_FNS/Dataset/valid', transform=validation_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=64, num_workers=4)\n",
    "\n",
    "test_accuracy = check_accuracy_on_test(test_dataloader, vgg16)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.class_idx_mapping = train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16, \"model_vgg16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "def load_model(model_checkpoint):\n",
    "    \"\"\"\n",
    "    Loads the model from a checkpoint file.\n",
    "\n",
    "    Arguments: \n",
    "        model_checkpoint: Path to checkpoint file\n",
    "    \n",
    "    Returns: \n",
    "        model: Loaded model.\n",
    "        idx_class_mapping: Index to class mapping for further evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = torch.load(model_checkpoint)\n",
    "    \n",
    "    #model = models.vgg16(pretrained=True)\n",
    "    \n",
    "    #for param in model.parameters():\n",
    "    #    param.requires_grad = False\n",
    "\n",
    "    #model.classifier = checkpoint[\"classifier\"]\n",
    "    #model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    #return model\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_checkpoint=\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx_mapping = train_dataset.class_to_idx\n",
    "idx_class_mapping = {v: k for k, v in class_idx_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model_checkpoint, topk=3, device=\"cpu\", idx_class_mapping=idx_class_mapping):\n",
    "    ''' \n",
    "    Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    \n",
    "    Arguments:\n",
    "        image_path: Path to the image\n",
    "        model: Trained model\n",
    "    Returns:\n",
    "        classes: Top k class numbers.\n",
    "        probs: Probabilities corresponding to those classes\n",
    "    '''\n",
    "    \n",
    "    # Build the model from the checkpoint\n",
    "    model = load_model(model_checkpoint)\n",
    "    \n",
    "    # No need for GPU\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "     \n",
    "    img = process_image(image_path)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img_tensor = torch.from_numpy(img).type(torch.FloatTensor).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        log_probabilities = model.forward(img_tensor)\n",
    "    \n",
    "    probabilities = torch.exp(log_probabilities)\n",
    "    probs, indices = probabilities.topk(topk)\n",
    "    \n",
    "    probs = probs.numpy().squeeze()\n",
    "    indices = indices.numpy().squeeze()\n",
    "    classes = [idx_class_mapping[index] for index in indices]\n",
    "    \n",
    "    return probs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_image(img_path):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    img = Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    if w<h:\n",
    "        size = 256, 999999999\n",
    "    else:\n",
    "        size = 999999999, 256\n",
    "\n",
    "    img.thumbnail(size=size)\n",
    "    \n",
    "    w, h = img.size\n",
    "    left = (w - 224) / 2\n",
    "    right = (w + 224) / 2\n",
    "    top = (h - 224) / 2\n",
    "    bottom = (h + 224) / 2\n",
    "    \n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    np_img = np.array(img)/255\n",
    "    \n",
    "    # Normalize\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_img = (np_img - mean) / std\n",
    "    \n",
    "    np_img = np_img.transpose(2, 0, 1)\n",
    "    \n",
    "    return np_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1', 1: '2', 2: '3'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, classes = predict(\"/home/a2-murat/Desktop/mnist_c++/Dataset/valid/3/Smoke_0_230.jpg\", \"model.pth\", idx_class_mapping = idx_class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9898845e-01 9.3350216e-04 7.7986348e-05]\n",
      "['3', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - model_trace was saved!\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "\n",
    "mnist_testset = datasets.ImageFolder(test_dir, transform=None)\n",
    "train_image, train_target= mnist_testset[20]\n",
    "\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = torch.load(\"C:/Users/PC/Desktop/Dataset_FNS/model_vgg16.pth\").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.CenterCrop(80),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tensor_image = loader(train_image).unsqueeze(0).to(device)\n",
    "\n",
    "traced_net = torch.jit.trace(model,tensor_image)\n",
    "traced_net.save(\"model_trace_vgg16.pt\")\n",
    "\n",
    "print(\"Success - model_trace was saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
